{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T14:34:54.358484Z",
     "start_time": "2025-05-07T14:19:39.314433Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fzkuji\\.conda\\envs\\ensemble\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\fzkuji\\.conda\\envs\\ensemble\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fzkuji\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-4B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Error while downloading from https://cdn-lfs-us-1.hf.co/repos/7b/f2/7bf2021b8b292041492926dd9f2a5d90a657978457bc7beaa9f07b62593574b1/6cd087b316306a68c562436b5492edbcf6e16c6dba3a1308279caa5a58e21ca5?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00003.safetensors%3B+filename%3D%22model-00002-of-00003.safetensors%22%3B&Expires=1746631192&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjYzMTE5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzdiL2YyLzdiZjIwMjFiOGIyOTIwNDE0OTI5MjZkZDlmMmE1ZDkwYTY1Nzk3ODQ1N2JjN2JlYWE5ZjA3YjYyNTkzNTc0YjEvNmNkMDg3YjMxNjMwNmE2OGM1NjI0MzZiNTQ5MmVkYmNmNmUxNmM2ZGJhM2ExMzA4Mjc5Y2FhNWE1OGUyMWNhNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=G%7EfaQhfDTOqsWcjrR5ecqHcgw0Z%7E5Z76FKV-YveT7hF-g1tsaArZjbqqIyhbFxi2kRk-KXXIUSqhUhAGjpo5V8v6QJzyEdjzYns3pMU%7E1k46rDuM6L8ZeokHOJapEg7mPmWM3LoLjBK83LG6GS4ttRBfGR50vBiwN5MK-LDDRMAF8vGsRrsJiObrUUqoYvWH-ReYSydZu4ElLTTcZL1VwwChb8qlDIqjy3vEsRevVzJqPtVtku6IaornXznHLlhzHonVWQtRhnx6iVpt5dzpvT94IJOb9S4a-Bp2aRQl-vZVlKdzUZinGMcpRijiQ-ABYEyFXj66mxRTjET1rvz7zA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/7b/f2/7bf2021b8b292041492926dd9f2a5d90a657978457bc7beaa9f07b62593574b1/6cd087b316306a68c562436b5492edbcf6e16c6dba3a1308279caa5a58e21ca5?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00003.safetensors%3B+filename%3D%22model-00002-of-00003.safetensors%22%3B&Expires=1746631192&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjYzMTE5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzdiL2YyLzdiZjIwMjFiOGIyOTIwNDE0OTI5MjZkZDlmMmE1ZDkwYTY1Nzk3ODQ1N2JjN2JlYWE5ZjA3YjYyNTkzNTc0YjEvNmNkMDg3YjMxNjMwNmE2OGM1NjI0MzZiNTQ5MmVkYmNmNmUxNmM2ZGJhM2ExMzA4Mjc5Y2FhNWE1OGUyMWNhNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=G%7EfaQhfDTOqsWcjrR5ecqHcgw0Z%7E5Z76FKV-YveT7hF-g1tsaArZjbqqIyhbFxi2kRk-KXXIUSqhUhAGjpo5V8v6QJzyEdjzYns3pMU%7E1k46rDuM6L8ZeokHOJapEg7mPmWM3LoLjBK83LG6GS4ttRBfGR50vBiwN5MK-LDDRMAF8vGsRrsJiObrUUqoYvWH-ReYSydZu4ElLTTcZL1VwwChb8qlDIqjy3vEsRevVzJqPtVtku6IaornXznHLlhzHonVWQtRhnx6iVpt5dzpvT94IJOb9S4a-Bp2aRQl-vZVlKdzUZinGMcpRijiQ-ABYEyFXj66mxRTjET1rvz7zA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Fetching 3 files: 100%|██████████| 3/3 [13:19<00:00, 266.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 33.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user asked for a short introduction to large language models. Let me start by defining what they are. I should mention that they're a type of AI model, specifically deep learning models. Maybe explain that they're trained on vast amounts of text data to understand and generate human-like text.\n",
      "\n",
      "I need to highlight their key capabilities: understanding context, generating text, answering questions, creating content. Also, mention their applications like customer service, content creation, coding, etc. But I should keep it concise. Maybe include examples like chatbots, virtual assistants, or writing tools.\n",
      "\n",
      "Wait, the user wants a short intro, so I shouldn't go into too much technical detail. Focus on the main points: what they are, how they work, their uses, and their impact. Also, maybe touch on the fact that they're developed by companies like Google, Microsoft, and others. But keep it brief. Avoid jargon. Make sure it's clear and easy to understand. Let me structure it step by step: definition, training process, capabilities, applications, and significance. That should cover it without being too long.\n",
      "</think>\n",
      "content: Large language models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. Trained on vast amounts of textual data, they can perform tasks such as answering questions, writing essays, coding, creating stories, and engaging in conversations. These models use deep learning techniques to recognize patterns and context in language, enabling them to produce coherent and contextually relevant responses. LLMs have revolutionized fields like customer service, content creation, and research, offering powerful tools for generating, summarizing, and analyzing text. Examples include models like GPT, BERT, and others developed by companies like Google, Microsoft, and Alibaba. Their ability to adapt to diverse tasks makes them a cornerstone of modern AI technology.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1152ea9971a71150"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
