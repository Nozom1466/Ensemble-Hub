{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "271f2cc3-85ac-4154-874c-f85c71e442cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook‑ready DeepSeek‑Qwen ensemble with proper **Qwen2.5‑Math‑PRM‑7B** reward\n",
    "===============================================================================\n",
    "\n",
    "* Two candidate generators (default: 1.5 B & 7 B Distill‑Qwen).  \n",
    "* Reward model now follows the **official step‑scoring recipe**: we load the RM\n",
    "  with `AutoModel`, pass the conversation through the chat template, identify\n",
    "  `<extra_0>` delimiters, and extract the probability of the *positive* label\n",
    "  at each step token.  The overall score is the **mean probability × 10** (so\n",
    "  it roughly ranges 0‑10 like a human grade).\n",
    "\n",
    "Usage in a notebook\n",
    "-------------------\n",
    "```python\n",
    "from ensemble_inference import run_ensemble\n",
    "print(run_ensemble(\"Explain gradient accumulation in simple terms.\"))\n",
    "```\n",
    "You can plug in extra candidate models by passing a list of paths to\n",
    "`run_ensemble(..., candidates=[...])`.\n",
    "\n",
    "Dependencies: `transformers >= 4.40`, `accelerate`, CUDA.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"ensemble_inference\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "\n",
    "STOP_TOKENS = {\".\", \"\\n\"}\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _trim_at_stop(text: str) -> str:\n",
    "    for t in STOP_TOKENS:\n",
    "        i = text.find(t)\n",
    "        if i != -1:\n",
    "            return text[: i + len(t)]\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Generator wrapper\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class HFModel:\n",
    "    name: str\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    model: PreTrainedModel\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, *, dtype: torch.dtype = torch.float16):\n",
    "        logger.info(\"Loading generator %s …\", path)\n",
    "        tok = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        mod = AutoModelForCausalLM.from_pretrained(\n",
    "            path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        ).eval()\n",
    "        return cls(path, tok, mod)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate_segment(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        *,\n",
    "        max_new_tokens: int = 128,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> str:\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        cfg = GenerationConfig(\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        out = self.model.generate(**ids, generation_config=cfg)[0]\n",
    "        decoded = self.tokenizer.decode(out[len(ids[\"input_ids\"][0]) :], skip_special_tokens=True)\n",
    "        return _trim_at_stop(decoded).strip()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Reward model wrapper (official step‑based scoring)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "STEP_TOKEN = \"<extra_0>\"\n",
    "SYSTEM_PROMPT = \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
    "\n",
    "\n",
    "def _make_step_rewards(logits: torch.Tensor, token_masks: torch.Tensor):\n",
    "    \"\"\"Return list of positive‑label probabilities for each step.\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1) * token_masks.unsqueeze(-1)  # B, T, C\n",
    "    results = []\n",
    "    for sample in probs:  # iterate batch (usually 1)\n",
    "        positive = sample[sample != 0].view(-1, 2)[:, 1]  # steps × 2 → take label‑1 prob\n",
    "        results.append(positive.cpu().tolist())\n",
    "    return results\n",
    "\n",
    "\n",
    "class PRMScorer:\n",
    "    \"\"\"Qwen2.5‑Math‑PRM‑7B scorer following the official implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, path: str, *, dtype: torch.dtype = torch.bfloat16):\n",
    "        logger.info(\"Loading reward model %s …\", path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        ).eval()\n",
    "        # Pre‑encode step separator id for masking\n",
    "        self.step_sep_id = self.tokenizer.encode(STEP_TOKEN)[0]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        # Ensure at least one step token at the end\n",
    "        if not answer.strip().endswith(STEP_TOKEN):\n",
    "            answer = answer.rstrip() + STEP_TOKEN\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        convo = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        input_ids = self.tokenizer(convo, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "        logits = self.model(input_ids=input_ids).logits  # (B, T, 2)\n",
    "        mask = input_ids == self.step_sep_id  # (B, T)\n",
    "        step_probs = _make_step_rewards(logits, mask)[0]  # list[float]\n",
    "\n",
    "        if not step_probs:  # fallback\n",
    "            return 0.0\n",
    "        return float(sum(step_probs) / len(step_probs) * 10.0)  # map to 0‑10\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Core ensemble loop\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class EnsembleReasoner:\n",
    "    candidate_models: List[HFModel]\n",
    "    scorer: PRMScorer\n",
    "    max_rounds: int = 5\n",
    "    score_threshold: float = 0.5\n",
    "\n",
    "    def __call__(self, question: str) -> Tuple[str, List[str]]:\n",
    "        context = question.strip()\n",
    "        chosen_segments: List[str] = []\n",
    "\n",
    "        for rnd in range(1, self.max_rounds + 1):\n",
    "            logger.info(\"⏩  Round %d\", rnd)\n",
    "            segments = [m.generate_segment(context) for m in self.candidate_models]\n",
    "            scores = [self.scorer.score(question, seg) for seg in segments]\n",
    "\n",
    "            for m, seg, sc in zip(self.candidate_models, segments, scores):\n",
    "                logger.info(\"→ %s | score %.2f | %s\", m.name, sc, seg.replace(\"\\n\", \"\\\\n\"))\n",
    "\n",
    "            best_idx = int(torch.tensor(scores).argmax())\n",
    "            best_score, best_seg = scores[best_idx], segments[best_idx]\n",
    "            logger.info(\"✅  Chosen: model=%s score=%.2f\", self.candidate_models[best_idx].name, best_score)\n",
    "\n",
    "            if best_score < self.score_threshold:\n",
    "                logger.info(\"Stopping early (score below threshold).\")\n",
    "                break\n",
    "\n",
    "            chosen_segments.append(best_seg)\n",
    "            context += \" \" + best_seg\n",
    "\n",
    "        return \" \".join(chosen_segments), chosen_segments\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Convenience functions\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_default_models(dtype: torch.dtype = torch.float16):\n",
    "    gens = [\n",
    "        \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    ]\n",
    "    candidates = [HFModel.load(p, dtype=dtype) for p in gens]\n",
    "    scorer = PRMScorer(\"/root/autodl-tmp/Qwen2.5-Math-PRM-7B\")\n",
    "    return candidates, scorer\n",
    "\n",
    "\n",
    "def run_ensemble(question: str, *, max_rounds: int = 5, score_threshold: float = 0.5) -> str:\n",
    "    candidates, scorer = load_default_models()\n",
    "    reasoner = EnsembleReasoner(candidates, scorer, max_rounds=max_rounds, score_threshold=score_threshold)\n",
    "    answer, _ = reasoner(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b55629-40f2-41d4-a410-13b1e23758fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loading generator /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B …\n",
      "[INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "[INFO] Loading generator /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B …\n",
      "[INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871c104daf794b31ac11a73d96976e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Loading reward model /root/autodl-tmp/Qwen2.5-Math-PRM-7B …\n",
      "[INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe64abc63b45443099dd6ecae9cd9886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO] ⏩  Round 1\n",
      "[WARNING] We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B | score 9.49 | Need to be careful about the wording.\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B | score 9.77 | For someone who has some basic knowledge of machine learning, how would you explain gradient accumulation? How is it different from gradient descent? What are the scenarios where gradient accumulation is useful?\\n\\nGradient accumulation is a technique used in training machine learning models, particularly deep learning models, that helps in managing memory usage and can be useful in certain scenarios.\n",
      "[INFO] ✅  Chosen: model=/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B score=9.77\n",
      "[INFO] ⏩  Round 2\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B | score 9.26 | It involves computing gradients across multiple examples or data points at once and storing these gradients, rather than computing them for each example separately.\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B | score 8.20 | It allows the training process to accumulate gradients over multiple batches before performing a single weight update, which can be beneficial for models that require more memory than is available.\n",
      "[INFO] ✅  Chosen: model=/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B score=9.26\n",
      "[INFO] ⏩  Round 3\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B | score 9.49 | This can be particularly beneficial when dealing with large datasets, as it reduces the memory required and can speed up training.\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B | score 8.83 | This allows the model to process larger batches of data without exceeding memory limits, which is especially helpful when dealing with large datasets or complex models.\n",
      "[INFO] ✅  Chosen: model=/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B score=9.49\n",
      "[INFO] ⏩  Round 4\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B | score 9.57 | The main idea is to compute the gradient for a batch of data, and then use that gradient to update the model's parameters.\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B | score 8.09 | Unlike gradient descent, which computes gradients for a single example at a time, gradient accumulation computes gradients for a batch and applies the update after processing a batch.\n",
      "[INFO] ✅  Chosen: model=/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B score=9.57\n",
      "[INFO] ⏩  Round 5\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-1.5B | score 8.01 | This is different from gradient descent, which computes the gradient for each individual example and uses it to update the model.\n",
      "[INFO] → /root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B | score 9.22 | This approach is different from traditional gradient descent, which computes gradients one example at a time, which can be slow and memory-intensive for large datasets.\n",
      "[INFO] ✅  Chosen: model=/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B score=9.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For someone who has some basic knowledge of machine learning, how would you explain gradient accumulation? How is it different from gradient descent? What are the scenarios where gradient accumulation is useful?\n",
      "\n",
      "Gradient accumulation is a technique used in training machine learning models, particularly deep learning models, that helps in managing memory usage and can be useful in certain scenarios. It involves computing gradients across multiple examples or data points at once and storing these gradients, rather than computing them for each example separately. This can be particularly beneficial when dealing with large datasets, as it reduces the memory required and can speed up training. The main idea is to compute the gradient for a batch of data, and then use that gradient to update the model's parameters. This approach is different from traditional gradient descent, which computes gradients one example at a time, which can be slow and memory-intensive for large datasets.\n"
     ]
    }
   ],
   "source": [
    "q = \"Explain gradient accumulation in simple terms.\"\n",
    "print(run_ensemble(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd9c88-dfd9-458d-b45d-3fa81d84929c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
