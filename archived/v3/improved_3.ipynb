{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "\n",
    "# Optional vLLM backend -----------------------------------------------------\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams  # type: ignore\n",
    "    _VLLM_AVAILABLE = True\n",
    "except ImportError:  # pragma: no cover\n",
    "    _VLLM_AVAILABLE = False\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Logging / constants\n",
    "# ---------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"ensemble_inference\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EOS_TEXT = \"\"  # Most Qwen / Llama models use empty string as EOS\n",
    "STEP_TOKEN = \"<extra_0>\"  # Token separator used by reward model\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "STOP_TOKENS_TEXT = {\".\", \"\\n\"}  # Stop decoding after these tokens\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Conversation Template\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ConversationTemplate:\n",
    "    \"\"\"\n",
    "    A conversation template for constructing dialogue prompts.\n",
    "    It includes a system prompt, a single user question, and accumulated assistant responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt: str, initial_question: str):\n",
    "        self.system = system_prompt\n",
    "        self.question = initial_question\n",
    "        self.assistant_parts: List[str] = []  # Collected assistant responses\n",
    "\n",
    "    def add_assistant(self, content: str):\n",
    "        \"\"\"Append a new assistant response to the prompt context.\"\"\"\n",
    "        self.assistant_parts.append(content.strip())\n",
    "\n",
    "    def render(self) -> str:\n",
    "        \"\"\"\n",
    "        Render the full prompt to be fed into a language model.\n",
    "        It includes the system message, user input, and accumulated assistant responses.\n",
    "        \"\"\"\n",
    "        lines = [\n",
    "            f\"[SYSTEM] {self.system} [/SYSTEM]\",\n",
    "            f\"<user>\\n{self.question.strip()}\\n</user>\",\n",
    "            f\"<assistant>\\n\" + \"\\n\".join(self.assistant_parts)\n",
    "        ]\n",
    "        return \"\".join(lines)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility: trim text at the last occurrence of stop tokens\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _trim_text(txt: str) -> str:\n",
    "    \"\"\"Truncate the text after the last known stop token for cleaner outputs.\"\"\"\n",
    "    best_pos = -1\n",
    "    best_tok = None\n",
    "    for tok in STOP_TOKENS_TEXT:\n",
    "        pos = txt.rfind(tok)\n",
    "        if pos > best_pos:\n",
    "            best_pos = pos\n",
    "            best_tok = tok\n",
    "    if best_pos != -1:\n",
    "        return txt[: best_pos + len(best_tok)]\n",
    "    return txt\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility: extract token-level reward scores from logits\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _step_rewards(logits: torch.Tensor, mask: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute step-wise probabilities using softmax over logits.\n",
    "    Only consider positions where mask is non-zero (STEP_TOKEN positions).\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=-1) * mask.unsqueeze(-1)\n",
    "    arr: List[List[float]] = []\n",
    "    for sample in probs:\n",
    "        pos = sample[sample != 0].view(-1, 2)[:, 1]\n",
    "        arr.append(pos.cpu().tolist())\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Output container for model generation\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GenOutput:\n",
    "    text: str\n",
    "    ended_with_eos: bool  # Whether EOS token was generated\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Abstract base class for any generator (HF or vLLM)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class BaseGenerator:\n",
    "    name: str\n",
    "\n",
    "    def generate(self, prompt: str, **kw) -> GenOutput:\n",
    "        \"\"\"Abstract method for generating model outputs.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HuggingFace Transformers-based Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class HFGenerator(BaseGenerator):\n",
    "    def __init__(self, path: str, *, device: str = \"auto\", dtype: torch.dtype = torch.bfloat16):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device,\n",
    "            trust_remote_code=True\n",
    "        ).eval()\n",
    "        self.name = path\n",
    "        self.device = next(self.model.parameters()).device if device == \"auto\" else torch.device(device)\n",
    "\n",
    "        # Optional stop string list\n",
    "        self.stop_strings = list(STOP_TOKENS_TEXT) + [\n",
    "            self.tokenizer.decode([self.tokenizer.eos_token_id], skip_special_tokens=False)\n",
    "        ]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, *, max_tokens=64, temperature=0.95, top_p=0.7) -> GenOutput:\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        cfg = GenerationConfig(\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        out = self.model.generate(**ids, generation_config=cfg, tokenizer=self.tokenizer)[0]\n",
    "        ended = bool(self.tokenizer.eos_token_id in out)\n",
    "        txt = self.tokenizer.decode(out[len(ids[\"input_ids\"][0]):], skip_special_tokens=False)\n",
    "        return GenOutput(_trim_text(txt) if not ended else txt, ended)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# vLLM-based Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class VLLMGenerator(BaseGenerator):\n",
    "    def __init__(self, path: str):\n",
    "        if not _VLLM_AVAILABLE:\n",
    "            raise RuntimeError(\"vLLM is not installed.\")\n",
    "        self._llm = LLM(model=path)\n",
    "        self._sp = SamplingParams(max_tokens=128, temperature=0.95, top_p=0.7, stop=list(STOP_TOKENS_TEXT))\n",
    "        self.name = path\n",
    "        self._eos_text = EOS_TEXT\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, *, max_tokens=30, temperature=0.95, top_p=0.7) -> GenOutput:\n",
    "        self._sp.max_tokens, self._sp.temperature, self._sp.top_p = max_tokens, temperature, top_p\n",
    "        txt = self._llm.generate([prompt], self._sp)[0].outputs[0].text\n",
    "        ended = txt.endswith(self._eos_text)\n",
    "        return GenOutput(_trim_text(txt), ended)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ModelPool: caches all loaded generators and reward models\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ModelPool:\n",
    "    _gen_cache: Dict[Tuple[str, str], BaseGenerator] = {}\n",
    "    _reward_cache: Dict[str, str] = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get_generator(cls, path: str, engine: str = \"hf\", device: Optional[str] = None) -> BaseGenerator:\n",
    "        \"\"\"\n",
    "        Load a generator model (e.g., HF or vLLM) to a specified device (e.g., 'cuda:0', 'cpu').\n",
    "        \"\"\"\n",
    "        key = (engine, path)\n",
    "        if key not in cls._gen_cache:\n",
    "            logger.info(\"[Pool] loading %s (%s)\", path, engine)\n",
    "\n",
    "            resolved_device = device or \"auto\"\n",
    "            logger.info(f\"→ Assigned to device: {resolved_device}\")\n",
    "\n",
    "            if engine == \"hf\":\n",
    "                cls._gen_cache[key] = HFGenerator(path, device=resolved_device)\n",
    "            elif engine == \"vllm\":\n",
    "                cls._gen_cache[key] = VLLMGenerator(path)  # vLLM usually uses global config\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown engine: {engine}\")\n",
    "        return cls._gen_cache[key]\n",
    "\n",
    "    @classmethod\n",
    "    def get_reward(cls, path: str, device: Optional[str] = None) -> \"PRMScorer\":\n",
    "        \"\"\"\n",
    "        Load a reward model to a specified device (e.g., 'cuda:0', 'cpu').\n",
    "        \"\"\"\n",
    "        if path not in cls._reward_cache:\n",
    "            logger.info(\"[Pool] loading reward model %s\", path)\n",
    "            resolved_device = device or \"auto\"\n",
    "            logger.info(f\"→ Reward model assigned to device: {resolved_device}\")\n",
    "            cls._reward_cache[path] = PRMScorer(path, device=resolved_device)\n",
    "        return cls._reward_cache[path]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PRMScorer: reward model used for evaluating step-level outputs\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class PRMScorer:\n",
    "    def __init__(self, path: str, device: str = \"auto\"):\n",
    "        self.tok = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        self.mod = AutoModel.from_pretrained(\n",
    "            path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            trust_remote_code=True\n",
    "        ).eval()\n",
    "        self.sep_id = self.tok.encode(STEP_TOKEN)[0]\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        \"\"\"Compute reward score from model output at STEP_TOKEN positions.\"\"\"\n",
    "        if not answer.endswith(STEP_TOKEN):\n",
    "            answer += STEP_TOKEN\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        convo = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "        ids = self.tok(convo, return_tensors=\"pt\").input_ids\n",
    "        mask = ids == self.sep_id\n",
    "        probs = _step_rewards(self.mod(ids).logits, mask)[0]\n",
    "        return float(sum(probs) / len(probs) * 10.0) if probs else 0.0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def score_batch_augmented(self, prompt: str, completions: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Efficiently score a batch of completions using the format:\n",
    "        [prompt + STEP_TOKEN + completion + STEP_TOKEN]\n",
    "        \"\"\"\n",
    "        inputs = [prompt + STEP_TOKEN + c + STEP_TOKEN for c in completions]\n",
    "        enc = self.tok(inputs, return_tensors=\"pt\", padding=True, truncation=True).to(self.mod.device)\n",
    "        mask = enc[\"input_ids\"] == self.sep_id\n",
    "        logits = self.mod(**enc).logits\n",
    "        probs = _step_rewards(logits, mask)\n",
    "        return [float(sum(p) / len(p) * 10.0) if p else 0.0 for p in probs]\n",
    "\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------------------------------------\n",
    "# EnsembleReasoner: multi-model decoding loop with step-wise reward scoring\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class EnsembleReasoner:\n",
    "    def __init__(self, generators: List[BaseGenerator], scorer: PRMScorer, max_rounds: int = 500,\n",
    "                 score_threshold: float = 0.5, accumulate_context: bool = True):\n",
    "        self.generators = generators\n",
    "        self.scorer = scorer\n",
    "        self.max_rounds = max_rounds\n",
    "        self.score_threshold = score_threshold\n",
    "        self.accumulate_context = accumulate_context\n",
    "\n",
    "    def __call__(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Iteratively decode using multiple generators.\n",
    "        In each round, the best candidate (with highest reward) is selected and appended.\n",
    "        Generation stops early if reward is low or EOS is emitted.\n",
    "        \"\"\"\n",
    "        convo = ConversationTemplate(SYSTEM_PROMPT, question)\n",
    "\n",
    "        for rnd in range(1, self.max_rounds + 1):\n",
    "            prompt = convo.render()\n",
    "\n",
    "            # Filter out generators that exceed input length\n",
    "            available_gens: List[BaseGenerator] = []\n",
    "            for g in self.generators:\n",
    "                tok = getattr(g, \"tokenizer\", None)\n",
    "                if tok is not None:\n",
    "                    length = tok(prompt, return_tensors=\"pt\").input_ids.size(1)\n",
    "                    if length > tok.model_max_length:\n",
    "                        logger.info(\"Skip %s: prompt length %d > max %d\",\n",
    "                                    g.name, length, tok.model_max_length)\n",
    "                        continue\n",
    "                available_gens.append(g)\n",
    "\n",
    "            if not available_gens:\n",
    "                logger.error(\"No generators available for current prompt length; stopping early.\")\n",
    "                break\n",
    "\n",
    "            # outs = [g.generate(prompt) for g in available_gens]\n",
    "            from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=len(available_gens)) as executor:\n",
    "                outs = list(executor.map(lambda g: g.generate(prompt), available_gens))\n",
    "\n",
    "            segs = [o.text for o in outs]\n",
    "\n",
    "            # Score each candidate using prompt + STEP_TOKEN + candidate + STEP_TOKEN\n",
    "            # scores = []\n",
    "            # for o in outs:\n",
    "            #     augmented = prompt + STEP_TOKEN + o.text + STEP_TOKEN\n",
    "            #     scores.append(self.scorer.score(question, augmented))\n",
    "            completions = [o.text for o in outs]\n",
    "            scores = self.scorer.score_batch_augmented(prompt, completions)\n",
    "            \n",
    "            \n",
    "            for g, t, s in zip(available_gens, segs, scores):\n",
    "                logger.info(f\"→ {g.name} | {s:.2f} | {t.replace(chr(10), '\\\\n')}\")\n",
    "\n",
    "            best_idx = int(torch.tensor(scores).argmax())\n",
    "            best_out = outs[best_idx]\n",
    "            best_score = scores[best_idx]\n",
    "\n",
    "            if best_score < self.score_threshold:\n",
    "                logger.info(\"Stop: best score %.2f < threshold\", best_score)\n",
    "                continue\n",
    "\n",
    "            convo.add_assistant(best_out.text)\n",
    "\n",
    "            if best_out.ended_with_eos:\n",
    "                logger.info(\"Early stop: EOS token emitted\")\n",
    "                break\n",
    "\n",
    "        # Return the final composed assistant response\n",
    "        return \"\\n\".join(convo.assistant_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee022b-5548-4da7-ba55-1159166bd18b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 第一版 直接调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac3983-1a58-4715-a5d5-49e4e90d012d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_specs = [\n",
    "    {\"path\": \"Qwen/Qwen2.5-Math-1.5B-Instruct\", \"engine\": \"hf\", \"device\": \"cuda:0\"},\n",
    "    {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \"engine\": \"hf\", \"device\": \"cuda:1\"},\n",
    "    # {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"engine\": \"hf\", \"device\": \"cuda:2\"},\n",
    "    # {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", \"engine\": \"hf\", \"device\": \"cuda:3\"},\n",
    "]\n",
    "\n",
    "reward_spec = {\"path\": \"Qwen/Qwen2.5-Math-PRM-7B\", \"device\": \"cuda:4\"}\n",
    "\n",
    "model_pool = ModelPool()\n",
    "gens = [\n",
    "    model_pool.get_generator(spec[\"path\"], spec.get(\"engine\", \"hf\"), spec.get(\"device\"))\n",
    "    for spec in model_specs\n",
    "]\n",
    "scorer = model_pool.get_reward(reward_spec[\"path\"], device=reward_spec[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abeb5d-0fae-4584-bde3-b84b926aab15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = r\"The expression $2\\cdot 3 \\cdot 4\\cdot 5+1$ is equal to 121, since multiplication is carried out before addition. However, we can obtain values other than 121 for this expression if we are allowed to change it by inserting parentheses. For example, we can obtain 144 by writing \\[ (2\\cdot (3\\cdot 4)) \\cdot (5+1) = 144. \\]In total, how many values can be obtained from the expression $2\\cdot 3\\cdot 4 \\cdot 5 + 1$ by inserting parentheses? (Note that rearranging terms is not allowed, only inserting parentheses).\"\n",
    "\n",
    "reasoner = EnsembleReasoner(gens, scorer, max_rounds=100, score_threshold=2.0, accumulate_context=True)\n",
    "\n",
    "response = reasoner(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f3295-1fb5-4242-be0f-c9c633002a4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 第二版 封装多模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b9242-f747-4e96-bd83-86effab68636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "def run_selective_ensemble(\n",
    "    question: str,\n",
    "    model_specs: List[Dict] = None,\n",
    "    reward_spec: Dict = None,\n",
    "    max_rounds: int = 500,\n",
    "    score_threshold: float = 0.5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Automatically selects the top-2 generators based on combined score\n",
    "    (low perplexity + high confidence), and performs ensemble reasoning.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question to answer.\n",
    "        model_specs (List[dict]): Each model must have 'path', 'engine', 'device'.\n",
    "        reward_spec (dict): Dict with keys 'path' and 'device'.\n",
    "        max_rounds (int): Maximum reasoning rounds for EnsembleReasoner.\n",
    "        score_threshold (float): Minimum score threshold to continue reasoning.\n",
    "\n",
    "    Returns:\n",
    "        str: Final answer generated by EnsembleReasoner using top-2 selected models.\n",
    "    \"\"\"\n",
    "    if model_specs is None:\n",
    "        model_specs = [\n",
    "            {\"path\": \"Qwen/Qwen2.5-Math-1.5B-Instruct\", \"engine\": \"hf\", \"device\": \"cuda:0\"},\n",
    "            {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \"engine\": \"hf\", \"device\": \"cuda:1\"},\n",
    "            {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"engine\": \"hf\", \"device\": \"cuda:2\"},\n",
    "            {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", \"engine\": \"hf\", \"device\": \"cuda:3\"},\n",
    "        ]\n",
    "\n",
    "    if reward_spec is None:\n",
    "        reward_spec = {\"path\": \"Qwen/Qwen2.5-Math-PRM-7B\", \"device\": \"cuda:4\"}\n",
    "\n",
    "    model_pool = ModelPool()\n",
    "    generators = []\n",
    "    scores = []\n",
    "\n",
    "    # Load generators from model specs\n",
    "    for spec in model_specs:\n",
    "        try:\n",
    "            gen = model_pool.get_generator(\n",
    "                path=spec[\"path\"],\n",
    "                engine=spec.get(\"engine\", \"hf\"),\n",
    "                device=spec.get(\"device\")\n",
    "            )\n",
    "            generators.append(gen)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load model {spec['path']}: {e}\")\n",
    "\n",
    "    # Load reward model\n",
    "    try:\n",
    "        scorer = model_pool.get_reward(\n",
    "            path=reward_spec[\"path\"],\n",
    "            device=reward_spec.get(\"device\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load reward model: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Score each generator using PPL + confidence\n",
    "    for gen in generators:\n",
    "        try:\n",
    "            convo = ConversationTemplate(SYSTEM_PROMPT, question)\n",
    "            prompt = convo.render()\n",
    "            tokenizer = getattr(gen, \"tokenizer\", None)\n",
    "            model = getattr(gen, \"model\", None)\n",
    "\n",
    "            if tokenizer is None or model is None:\n",
    "                logger.warning(f\"Skipping {gen.name}: missing tokenizer or model\")\n",
    "                continue\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(gen.device)\n",
    "                outputs = model(**inputs, output_attentions=False, output_hidden_states=False)\n",
    "                logits = outputs.logits[:, :-1, :]\n",
    "                labels = inputs[\"input_ids\"][:, 1:]\n",
    "\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                token_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                mask = labels != tokenizer.pad_token_id\n",
    "                token_log_probs = token_log_probs[mask]\n",
    "\n",
    "                avg_nll = -token_log_probs.mean().item()\n",
    "                perplexity = math.exp(avg_nll)\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                max_probs = probs.max(dim=-1).values.squeeze(0)\n",
    "                mask_flat = mask.squeeze(0)\n",
    "                confidence = max_probs[mask_flat].mean().item()\n",
    "\n",
    "                combined_score = -perplexity + confidence\n",
    "                scores.append((combined_score, gen))\n",
    "\n",
    "                logger.info(f\"{gen.name} | PPL: {perplexity:.2f} | Confidence: {confidence:.2f} | Score: {combined_score:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error scoring model {gen.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(scores) < 2:\n",
    "        logger.error(\"Not enough valid models to run ensemble reasoning.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Select top-2 generators\n",
    "    top_gens = sorted(scores, key=lambda x: x[0], reverse=True)[:2]\n",
    "    selected_generators = [item[1] for item in top_gens]\n",
    "    logger.info(f\"Selected models: {[g.name for g in selected_generators]}\")\n",
    "\n",
    "    # Run ensemble reasoning\n",
    "    reasoner = EnsembleReasoner(\n",
    "        generators=selected_generators,\n",
    "        scorer=scorer,\n",
    "        max_rounds=max_rounds,\n",
    "        score_threshold=score_threshold\n",
    "    )\n",
    "    return reasoner(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32065c2d-85db-4307-b9e9-5b74cebc7e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = r\"The expression $2\\cdot 3 \\cdot 4\\cdot 5+1$ is equal to 121, since multiplication is carried out before addition. However, we can obtain values other than 121 for this expression if we are allowed to change it by inserting parentheses. For example, we can obtain 144 by writing \\[ (2\\cdot (3\\cdot 4)) \\cdot (5+1) = 144. \\]In total, how many values can be obtained from the expression $2\\cdot 3\\cdot 4 \\cdot 5 + 1$ by inserting parentheses? (Note that rearranging terms is not allowed, only inserting parentheses).\"\n",
    "\n",
    "answer = run_selective_ensemble(text)\n",
    "print(\"Final Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09bb11-a050-44e6-8c64-cd82e1a13524",
   "metadata": {},
   "source": [
    "### 第三版 完整ensemble调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca221f-e96c-4309-ba51-efe25032722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Dict, Callable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from loguru import logger\n",
    "\n",
    "# Assumes SYSTEM_PROMPT and ConversationTemplate already defined\n",
    "\n",
    "class ModelStatStore:\n",
    "    def __init__(self):\n",
    "        self._stats: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    def has(self, model_path: str) -> bool:\n",
    "        return model_path in self._stats\n",
    "\n",
    "    def get(self, model_path: str) -> Dict[str, float]:\n",
    "        return self._stats[model_path]\n",
    "\n",
    "    def set(self, model_path: str, stats: Dict[str, float]):\n",
    "        self._stats[model_path] = stats\n",
    "\n",
    "    def maybe_compute(self, model_path: str, model, tokenizer, device, dataset: List[str]):\n",
    "        if not self.has(model_path):\n",
    "            stats = compute_model_stats_on_dataset(model, tokenizer, device, dataset)\n",
    "            self.set(model_path, stats)\n",
    "        return self.get(model_path)\n",
    "\n",
    "def compute_model_stats_on_dataset(model, tokenizer, device, dataset: List[str]) -> Dict[str, float]:\n",
    "    all_ppls, all_confs = [], []\n",
    "    for problem in dataset:\n",
    "        inputs = tokenizer(problem, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, :-1, :]\n",
    "            labels = inputs[\"input_ids\"][:, 1:]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "            mask = labels != tokenizer.pad_token_id\n",
    "            token_log_probs = token_log_probs[mask]\n",
    "            avg_nll = -token_log_probs.mean().item()\n",
    "            perplexity = math.exp(avg_nll)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            max_probs = probs.max(dim=-1).values.squeeze(0)\n",
    "            mask_flat = mask.squeeze(0)\n",
    "            confidence = max_probs[mask_flat].mean().item()\n",
    "\n",
    "            all_ppls.append(perplexity)\n",
    "            all_confs.append(confidence)\n",
    "\n",
    "    return {\n",
    "        \"ppl_mean\": float(torch.tensor(all_ppls).mean()),\n",
    "        \"ppl_std\": float(torch.tensor(all_ppls).std()),\n",
    "        \"conf_mean\": float(torch.tensor(all_confs).mean()),\n",
    "        \"conf_std\": float(torch.tensor(all_confs).std()),\n",
    "    }\n",
    "\n",
    "def score_question_for_model(question: str, model, tokenizer, device: str, prompt_builder: Callable) -> Dict[str, float]:\n",
    "    prompt = prompt_builder(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = inputs[\"input_ids\"][:, 1:]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "        mask = labels != tokenizer.pad_token_id\n",
    "        token_log_probs = token_log_probs[mask]\n",
    "        avg_nll = -token_log_probs.mean().item()\n",
    "        ppl = math.exp(avg_nll)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        max_probs = probs.max(dim=-1).values.squeeze(0)\n",
    "        mask_flat = mask.squeeze(0)\n",
    "        conf = max_probs[mask_flat].mean().item()\n",
    "\n",
    "    return {\"ppl\": ppl, \"conf\": conf}\n",
    "\n",
    "def determine_model_count(question_scores: List[Dict[str, float]], model_stats: Dict[str, Dict[str, float]]) -> int:\n",
    "    over_threshold = 0\n",
    "    for score, (model_path, stats) in zip(question_scores, model_stats.items()):\n",
    "        if score[\"ppl\"] > stats[\"ppl_mean\"] + 2:\n",
    "            over_threshold += 1\n",
    "    if over_threshold >= len(question_scores) * 0.90:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def select_top_models_by_z_score(question: str, model_specs: List[Dict], prompt_builder, model_stats: Dict[str, Dict[str, float]], model_pool, model_count: int = -1) -> List[Dict]:\n",
    "    results = []\n",
    "    question_scores = []\n",
    "    for spec in model_specs:\n",
    "        model = model_pool.get_generator(spec[\"path\"], spec.get(\"engine\", \"hf\"), spec.get(\"device\")).model\n",
    "        tokenizer = model_pool.get_generator(spec[\"path\"], spec.get(\"engine\", \"hf\"), spec.get(\"device\")).tokenizer\n",
    "        score = score_question_for_model(question, model, tokenizer, spec[\"device\"], prompt_builder)\n",
    "        stats = model_stats[spec[\"path\"]]\n",
    "        z_ppl = (stats[\"ppl_mean\"] - score[\"ppl\"]) / stats[\"ppl_std\"]\n",
    "        z_conf = (score[\"conf\"] - stats[\"conf_mean\"]) / stats[\"conf_std\"]\n",
    "        total_score = z_ppl + z_conf\n",
    "        results.append((total_score, spec))\n",
    "        question_scores.append(score)\n",
    "\n",
    "    if model_count == -1:\n",
    "        model_count = determine_model_count(question_scores, model_stats)\n",
    "\n",
    "    results = sorted(results, key=lambda x: x[0], reverse=True)\n",
    "    return [spec for _, spec in results[:model_count]]\n",
    "\n",
    "def run_zscore_ensemble(\n",
    "    question: str,\n",
    "    dataset_problems: List[str],\n",
    "    model_specs: List[Dict],\n",
    "    reward_spec: Dict,\n",
    "    stat_store: ModelStatStore,\n",
    "    max_rounds: int = 500,\n",
    "    score_threshold: float = 0.5\n",
    ") -> str:\n",
    "\n",
    "    logger.info(\"[Stage 1] Computing or retrieving reference statistics for all models...\")\n",
    "    model_pool = ModelPool()\n",
    "    model_stats = {}\n",
    "    for spec in model_specs:\n",
    "        model_path = spec[\"path\"]\n",
    "        generator = model_pool.get_generator(spec[\"path\"], spec.get(\"engine\", \"hf\"), spec.get(\"device\"))\n",
    "        stats = stat_store.maybe_compute(model_path, generator.model, generator.tokenizer, generator.device, dataset_problems)\n",
    "        model_stats[model_path] = stats\n",
    "        logger.info(\n",
    "            f\"→ Stats for {model_path}: \"\n",
    "            f\"PPL µ={stats['ppl_mean']:.2f}, σ={stats['ppl_std']:.2f} | \"\n",
    "            f\"Conf µ={stats['conf_mean']:.2f}, σ={stats['conf_std']:.2f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "    logger.info(\"[Stage 2] Selecting top models based on z-score (auto model count)...\")\n",
    "    prompt_builder = lambda q: ConversationTemplate(SYSTEM_PROMPT, q).render()\n",
    "    selected_specs = select_top_models_by_z_score(\n",
    "        question=question,\n",
    "        model_specs=model_specs,\n",
    "        prompt_builder=prompt_builder,\n",
    "        model_stats=model_stats,\n",
    "        model_pool=model_pool,\n",
    "        model_count=-1\n",
    "    )\n",
    "    logger.info(f\"✅ Selected models: {[s['path'] for s in selected_specs]}\")\n",
    "\n",
    "    logger.info(\"[Stage 3] Loading selected generators and reward model...\")\n",
    "    generators = [\n",
    "        model_pool.get_generator(spec[\"path\"], spec.get(\"engine\", \"hf\"), spec.get(\"device\"))\n",
    "        for spec in selected_specs\n",
    "    ]\n",
    "    scorer = model_pool.get_reward(reward_spec[\"path\"], device=reward_spec[\"device\"])\n",
    "\n",
    "    logger.info(\"[Stage 4] Running ensemble reasoner...\")\n",
    "    reasoner = EnsembleReasoner(\n",
    "        generators=generators,\n",
    "        scorer=scorer,\n",
    "        max_rounds=max_rounds,\n",
    "        score_threshold=score_threshold\n",
    "    )\n",
    "    return reasoner(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131fa80-8e55-426f-9ef8-7b126bdd64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 初始化模型统计缓存器\n",
    "stat_store = ModelStatStore()\n",
    "\n",
    "# 加载 MATH-500 数据集问题列表（只提取 math 问题本身）\n",
    "math_dataset = load_dataset(\"/mnt/data/zichuanfu/.cache/huggingface/hub/datasets--HuggingFaceH4--MATH-500/snapshots/ff5b20257d8185524591543f8ff5993951537bb8\", split=\"test\")\n",
    "math_problems = [x[\"problem\"] for x in math_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5ee19-9cd3-4f91-943b-0063e6095903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_specs = [\n",
    "#     {\"path\": \"Qwen/Qwen2.5-Math-1.5B-Instruct\", \"engine\": \"hf\", \"device\": \"cuda:0\"},\n",
    "#     {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \"engine\": \"hf\", \"device\": \"cuda:1\"},\n",
    "#     {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"engine\": \"hf\", \"device\": \"cuda:2\"},\n",
    "#     {\"path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", \"engine\": \"hf\", \"device\": \"cuda:3\"},\n",
    "# ]\n",
    "\n",
    "model_specs = [\n",
    "    {\"path\": \"/mnt/data/zichuanfu/.cache/huggingface/hub/models--Qwen--Qwen2.5-Math-1.5B-Instruct/snapshots/aafeb0fc6f22cbf0eaeed126eff8be45b0360a35\", \"engine\": \"hf\", \"device\": \"cuda:0\"},\n",
    "    {\"path\": \"/mnt/data/zichuanfu/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\", \"engine\": \"hf\", \"device\": \"cuda:1\"},\n",
    "    {\"path\": \"/mnt/data/zichuanfu/.cache/huggingface/hub/models--Qwen--Qwen2.5-Math-7B-Instruct/snapshots/ef9926d75ab1d54532f6a30dd5e760355eb9aa4d\", \"engine\": \"hf\", \"device\": \"cuda:2\"},\n",
    "    {\"path\": \"/mnt/data/zichuanfu/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60\", \"engine\": \"hf\", \"device\": \"cuda:3\"},\n",
    "    {\"path\": \"/mnt/data/zichuanfu/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-14B/snapshots/1df8507178afcc1bef68cd8c393f61a886323761\", \"engine\": \"hf\", \"device\": \"cuda:4\"},\n",
    "]\n",
    "\n",
    "reward_spec = {\"path\": \"/mnt/data/zichuanfu/.cache/huggingface/hub/models--Qwen--Qwen2.5-Math-PRM-7B/snapshots/0610740060112df12585d00a1c5f4624d2f59051\", \"device\": \"cuda:5\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295add5-a426-477e-81a6-9a69df9deb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_answer = run_zscore_ensemble(\n",
    "    question=\"If x^2 = 49, what is the positive value of x?\",\n",
    "    dataset_problems=math_problems,\n",
    "    model_specs=model_specs,\n",
    "    reward_spec=reward_spec,\n",
    "    stat_store=stat_store\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\\n\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98200bd-1813-40f2-b833-e89bdbb6a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r\"The expression $2\\cdot 3 \\cdot 4\\cdot 5+1$ is equal to 121, since multiplication is carried out before addition. However, we can obtain values other than 121 for this expression if we are allowed to change it by inserting parentheses. For example, we can obtain 144 by writing \\[ (2\\cdot (3\\cdot 4)) \\cdot (5+1) = 144. \\]In total, how many values can be obtained from the expression $2\\cdot 3\\cdot 4 \\cdot 5 + 1$ by inserting parentheses? (Note that rearranging terms is not allowed, only inserting parentheses).\"\n",
    "\n",
    "final_answer = run_zscore_ensemble(\n",
    "    question=text,\n",
    "    dataset_problems=math_problems,\n",
    "    model_specs=model_specs,\n",
    "    reward_spec=reward_spec,\n",
    "    stat_store=stat_store\n",
    ")\n",
    "print(\"Final Answer:\\n\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30267c29-a9f4-4fb2-bd8b-581e3b1f9d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设这些模块你都已经加载好了\n",
    "# from your_module import run_zscore_ensemble, ModelStatStore, model_specs, reward_spec\n",
    "\n",
    "def load_dataset(input_path: str) -> list:\n",
    "    \"\"\"加载 JSON 格式的数据集\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_predictions(predictions: list, output_path: str):\n",
    "    \"\"\"将推理结果保存为 JSONL 格式\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in predictions:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def run_batch_inference(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    model_specs: list,\n",
    "    reward_spec: dict,\n",
    "    math_problem_stats: list,\n",
    "    max_examples: int = None\n",
    "):\n",
    "    dataset = load_dataset(input_path)\n",
    "    stat_store = ModelStatStore()\n",
    "\n",
    "    predictions = []\n",
    "    for example in tqdm(dataset[:max_examples] if max_examples else dataset):\n",
    "        instruction = example[\"instruction\"].strip()\n",
    "        question = example[\"input\"].strip()\n",
    "        answer = example[\"output\"].strip()\n",
    "\n",
    "        # 构建推理输入\n",
    "        prompt = f\"\\n{instruction}\\n{question}\\nassistant\\n\"\n",
    "\n",
    "        # 调用推理函数\n",
    "        try:\n",
    "            result = run_zscore_ensemble(\n",
    "                question=question,\n",
    "                dataset_problems=math_problem_stats,\n",
    "                model_specs=model_specs,\n",
    "                reward_spec=reward_spec,\n",
    "                stat_store=stat_store\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on question: {question[:80]}... -> {e}\")\n",
    "            result = \"\"\n",
    "\n",
    "        predictions.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"predict\": result.strip(),\n",
    "            \"label\": answer.strip()\n",
    "        })\n",
    "\n",
    "    save_predictions(predictions, output_path)\n",
    "    print(f\"✅ Saved {len(predictions)} predictions to {output_path}\")\n",
    "\n",
    "\n",
    "# 示例调用：\n",
    "run_batch_inference(\n",
    "    input_path=\"/mnt/data/zichuanfu/LLaMA-Factory/data/hendrycks_math/train.json\",\n",
    "    output_path=\"deepseek-r1-1.5b-generated-predictions.jsonl\",\n",
    "    model_specs=model_specs,\n",
    "    reward_spec=reward_spec,\n",
    "    math_problem_stats=math_problems,  # 这个你已经提前加载过\n",
    "    max_examples=100  # 可选限制数量\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0169e-faca-4730-94f0-2f5f7d1f767f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensemble",
   "language": "python",
   "name": "ensemble"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
