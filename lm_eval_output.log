2025-05-27:10:47:24 DEBUG    [tasks:539] File _evalita-mp_ner_adg.yaml in /Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/tasks/evalita_llm could not be loaded
2025-05-27:10:47:24 DEBUG    [tasks:539] File _evalita-mp_ner_wn.yaml in /Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/tasks/evalita_llm could not be loaded
2025-05-27:10:47:24 DEBUG    [tasks:539] File _evalita-mp_ner_fic.yaml in /Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/tasks/evalita_llm could not be loaded
2025-05-27:10:47:25 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-05-27:10:47:25 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-05-27:10:47:25 INFO     [evaluator:223] Initializing openai-completions model, with arguments: {'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'base_url': 'http://localhost:8000/v1/chat/completions', 'tokenizer_backend': 'None'}
2025-05-27:10:47:25 WARNING  [models.api_models:160] Batch size > 1 detected. Ensure your API supports batched requests with varying total sequence lengths.
2025-05-27:10:47:25 INFO     [models.api_models:168] Using max length 2048 - 1
2025-05-27:10:47:25 INFO     [models.api_models:171] Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.
2025-05-27:10:47:25 INFO     [models.api_models:187] Using tokenizer None
2025-05-27:10:47:37 INFO     [evaluator:286] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
2025-05-27:10:47:37 WARNING  [evaluator:305] Overwriting default num_fewshot of gsm8k from 5 to 5
2025-05-27:10:47:37 INFO     [api.task:434] Building contexts for gsm8k on rank 0...
Selected Tasks: ['gsm8k']
  0%|          | 0/5 [00:00<?, ?it/s]100%|██████████| 5/5 [00:00<00:00, 781.47it/s]
2025-05-27:10:47:37 DEBUG    [evaluator:530] Task: gsm8k; number of requests on this rank: 5
2025-05-27:10:47:37 INFO     [evaluator:559] Running generate_until requests
2025-05-27:10:47:37 INFO     [models.api_models:681] Tokenized requests are disabled. Context + generation length is not checked.
Requesting API:   0%|          | 0/5 [00:00<?, ?it/s]2025-05-27:10:47:37 WARNING  [models.api_models:349] Cannot determine EOS string to pass to stop sequence. Manually set by passing `eos_string` to model_args.
Traceback (most recent call last):
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/http/client.py", line 1333, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/http/client.py", line 1093, in _send_output
    self.send(msg)
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/http/client.py", line 1037, in send
    self.connect()
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x16be73110>: Failed to establish a new connection: [Errno 61] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x16be73110>: Failed to establish a new connection: [Errno 61] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/miniconda3/envs/ensemble/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
             ^^^^^^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/evaluator.py", line 338, in simple_evaluate
    results = evaluate(
              ^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/evaluator.py", line 570, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/models/api_models.py", line 704, in generate_until
    outputs = retry(
              ^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/tenacity/__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/tenacity/__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/Users/fzkuji/PycharmProjects/lm-evaluation-harness/lm_eval/models/api_models.py", line 422, in model_call
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/ensemble/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x16be73110>: Failed to establish a new connection: [Errno 61] Connection refused'))
Requesting API:   0%|          | 0/5 [00:02<?, ?it/s]
